{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMSINQXzRM9K"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the training challenges and solutions\n",
        "with open('arc-agi_training_challenges.json', 'r') as file:\n",
        "    training_challenges = json.load(file)\n",
        "\n",
        "with open('arc-agi_training_solutions.json', 'r') as file:\n",
        "    training_solutions = json.load(file)\n",
        "\n",
        "# Load the evaluation challenges and solutions\n",
        "with open('arc-agi_evaluation_challenges.json', 'r') as file:\n",
        "    evaluation_challenges = json.load(file)\n",
        "\n",
        "with open('arc-agi_evaluation_solutions.json', 'r') as file:\n",
        "    evaluation_solutions = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import json\n",
        "\n",
        "# Define the padding function\n",
        "def pad_grid(grid, target_size=(30, 30)):\n",
        "    if grid.ndim != 2:\n",
        "        raise ValueError(f\"Grid should have 2 dimensions, but has {grid.ndim}\")\n",
        "    padded_grid = np.zeros(target_size, dtype=int)\n",
        "    padded_grid[:grid.shape[0], :grid.shape[1]] = grid\n",
        "    return padded_grid\n",
        "\n",
        "# Define the dataset class\n",
        "class ARCDataset(Dataset):\n",
        "    def __init__(self, challenges, solutions):\n",
        "        self.challenges = challenges\n",
        "        self.solutions = solutions\n",
        "        self.keys = list(challenges.keys())\n",
        "        self.valid_keys = self._filter_valid_keys()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        key = self.valid_keys[idx]\n",
        "        challenge = self.challenges[key]\n",
        "        solution = self.solutions[key]\n",
        "\n",
        "        # Extracting and padding the train and test grids\n",
        "        train_inputs = [pad_grid(np.array(pair['input'])) for pair in challenge['train']]\n",
        "        train_outputs = [pad_grid(np.array(pair['output'])) for pair in challenge['train']]\n",
        "        test_inputs = [pad_grid(np.array(pair['input'])) for pair in challenge['test']]\n",
        "        test_outputs = [pad_grid(np.array(output)) for output in solution]\n",
        "\n",
        "        # Convert to tensors\n",
        "        train_inputs = torch.tensor(train_inputs, dtype=torch.float32)\n",
        "        train_outputs = torch.tensor(train_outputs, dtype=torch.float32)\n",
        "        test_inputs = torch.tensor(test_inputs, dtype=torch.float32)\n",
        "        test_outputs = torch.tensor(test_outputs, dtype=torch.float32)\n",
        "\n",
        "        # Ensure the inputs have the correct dimensions\n",
        "        train_inputs = train_inputs.unsqueeze(1)\n",
        "        train_outputs = train_outputs.unsqueeze(1)\n",
        "        test_inputs = test_inputs.unsqueeze(1)\n",
        "        test_outputs = test_outputs.unsqueeze(1)\n",
        "\n",
        "        return train_inputs, train_outputs, test_inputs, test_outputs\n",
        "\n",
        "    def _filter_valid_keys(self):\n",
        "        valid_keys = []\n",
        "        for key in self.keys:\n",
        "            challenge = self.challenges[key]\n",
        "            if all('input' in pair and np.array(pair['input']).ndim == 2 for pair in challenge['train']) and \\\n",
        "               all('output' in pair and np.array(pair['output']).ndim == 2 for pair in challenge['train']) and \\\n",
        "               all('input' in pair and np.array(pair['input']).ndim == 2 for pair in challenge['test']):\n",
        "                valid_keys.append(key)\n",
        "            else:\n",
        "                print(f\"Skipping challenge {key} due to invalid grid dimensions\")\n",
        "        return valid_keys\n",
        "\n",
        "# Load data from JSON files\n",
        "with open('arc-agi_training_challenges.json') as f:\n",
        "    training_challenges = json.load(f)\n",
        "with open('arc-agi_training_solutions.json') as f:\n",
        "    training_solutions = json.load(f)\n",
        "with open('arc-agi_evaluation_challenges.json') as f:\n",
        "    evaluation_challenges = json.load(f)\n",
        "with open('arc-agi_evaluation_solutions.json') as f:\n",
        "    evaluation_solutions = json.load(f)\n",
        "with open('arc-agi_test_challenges.json') as f:\n",
        "    test_challenges = json.load(f)\n",
        "\n",
        "# Create DataLoader for training data\n",
        "train_dataset = ARCDataset(training_challenges, training_solutions)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Define the neural network model\n",
        "class ARCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ARCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 30 * 30, 512)\n",
        "        self.fc2 = nn.Linear(512, 30 * 30)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_pairs, _, height, width = x.size()\n",
        "        x = x.view(batch_size * num_pairs, 1, height, width)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = x.view(batch_size, num_pairs, 1, height, width)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, define loss function and optimizer\n",
        "model = ARCNN()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for train_inputs, train_outputs, _, _ in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_inputs)\n",
        "        target_size = outputs.size()\n",
        "        train_outputs = train_outputs.view(target_size)\n",
        "        loss = criterion(outputs, train_outputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOc1wNRKdAEW",
        "outputId": "4332802b-416f-4df8-8295-0ae742ccf776"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.195794\n",
            "Epoch [2/10], Loss: 1.054697\n",
            "Epoch [3/10], Loss: 0.904320\n",
            "Epoch [4/10], Loss: 0.779642\n",
            "Epoch [5/10], Loss: 0.750139\n",
            "Epoch [6/10], Loss: 0.708998\n",
            "Epoch [7/10], Loss: 0.618955\n",
            "Epoch [8/10], Loss: 0.544355\n",
            "Epoch [9/10], Loss: 0.495360\n",
            "Epoch [10/10], Loss: 0.463073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for test_inputs, test_outputs, _, _ in data_loader:\n",
        "            outputs = model(test_inputs)\n",
        "            target_size = outputs.size()\n",
        "            test_outputs = test_outputs.view(target_size)\n",
        "            loss = criterion(outputs, test_outputs)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# Create DataLoader for evaluation data\n",
        "evaluation_dataset = ARCDataset(evaluation_challenges, evaluation_solutions)\n",
        "evaluation_loader = DataLoader(evaluation_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluation_loss = evaluate_model(model, evaluation_loader)\n",
        "print(f'Evaluation Loss: {evaluation_loss:.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdpQ_UhPodg7",
        "outputId": "52372026-afb8-4fe2-d306-004701755a67"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Loss: 1.784493\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}